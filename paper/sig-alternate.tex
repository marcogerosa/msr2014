\documentclass{sig-alternate}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{flushend}

\lstset{
language=Java,
basicstyle=\footnotesize,
showtabs=false, 
showspaces=false,
showstringspaces=false,
frame=single,
breaklines=true,
tabsize=2,
captionpos=b
}

\begin{document}

\conferenceinfo{MSR}{Mining Software Repositories '14}

\title{Calculating Code Coverage Without Running the Tests:\\
A Heuristic Based on Static Code Analysis}

\numberofauthors{1}
\author{
\alignauthor
Mauricio F. Aniche, Gustavo A. Oliva, Marco A. Gerosa\\
\affaddr{Computer Science Department / IME}\\
\affaddr{University of São Paulo}\\
\email{\{aniche, goliva, gerosa\}@ime.usp.br}
}

\maketitle
\begin{abstract}

Code coverage is an important metric for software evolution and analysis.
However, computing it requires building the project and executing its test suite.
This is unfeasible for large-scale mining software repositories studies. To this end,
we conceived and implemented a heuristic to calculate code coverage based on 
code static analysis. In this paper, we describe the heuristic, its implementation, 
and a preliminary evaluation on three industry projects. We evaluated our implementation by 
comparing its results to those produced by the popular code coverage tool called Emma. 
Preliminary results showed that our approach has acceptable results in 2 out of 
the 3 analyzed projects.
These promising results showed that used and further extended for mining studies.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

Version control systems store software source code, which is often easy to extract but hard to process. 
Compiling code is not a straightforward task. 
For instance, each project has its own build process and might require external libraries. Furthermore, 
it is common that many of these libraries are not available by the time 
that researchers obtain the source code. Therefore, compiling and executing 
unit tests may not be feasible when conducting a large scale mining software repository study.
This may cause problems when metrics 
require the code to be compiled first. 

An example of such metric is \textbf{code coverage}. 
This metric defines the percentage of production code that is covered 
by the test suite. To enable the calculation of code coverage for mining software
repository studies, we conceived a heuristic that does not require compiled code
and neither requires the test suite to be run. 

This heuristic uses McCabe's number \cite{mccabe} and the number of unit tests to estimate code coverage. We discuss in this paper its implementation and its evaluation on three industry projects. 

This paper is structured as follows. In Section \ref{sec-code-coverage}, 
we introduce the concept of code coverage. In Section \ref{sec-heuristic}, 
we present our heuristic. In Section \ref{sec-experiment}, we describe the 
experiment design. In Section \ref{poc}, we describe a proof of concept that
we have done prior to implement the static tool. 
In Section \ref{sec-results}, we present the results we 
obtained and discuss them. In Section \ref{sec-threats}, 
we present the threats that may have influenced the validity of our study. 
In Section \ref{sec-related-work}, we show related studies and compare them 
to our approach. Finally, in Section \ref{sec-conclusion}, we state our 
conclusions and plans for future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
\label{sec-code-coverage}

% Gustavo: tentei mostrar estudos relacionados aqui, mas acho que poderia ter vendido melhor
Code coverage measures the degree to which production code is tested by a test suite \cite{code-coverage}. 
When a unit test exercises a piece of code, developers say that such piece
of code is covered. Many studies argue that code coverage is an important
metric when dealing with software maintenance and evolution \cite{sebastian} \cite{del-frate} \cite{mei-hwa}.

The common output of the tools that implement such metric is a percentage value, 
which indicates the fraction of production code lines, instructions, 
or even execution branches that are exercised by the test suite. Such information are shown according to different
perspectives, such as per package, compilation unit, class, or method. 

Emma \footnote{http://emma.sourceforge.net} and Cobertura\footnote{http://cobertura.github.io/cobertura} are popular 
code coverage tools. Both of them use dynamic analysis to calculate the metric. They instrument the code, execute 
the unit test suite, and then calculate the degree to which the production code was exercised by the test suite. 
These tools have two main problems: (i) depending on the size of the test suite, it can take a long time to compute
the metric, and (ii) they require compiled code, and as we said before, compiling large amount of projects is not trivial. Researchers then need an alternative way to calculate code coverage.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Our Approach}
\label{sec-heuristic}

Our heuristic is based on the idea that if a production method has N different branches, then it should
have at least N associated unit tests. The first step is to estimate the number of branches 
a production class has. To do that, we use McCabe's cyclomatic complexity \cite{mccabe}, a popular
metric both on academia and industry.
McCabe's number shows the different number of execution paths per method. 
As we are calculating code coverage in class methods, we calculate the class' cyclomatic complexity
by summing up McCabe's number of all methods.

The next step was to count the number of unit tests that invoke each production method. To accomplish that
in a static way, we search for production method invocations that occur inside each unit test. As an example, in Listing
\ref{unit-test}, the invocation \textit{inv.calcTaxes()} links the surrounding unit test (\textit{shouldCalculateTaxes()}) to 
the production method \textit{calcTaxes(double increment)} from the class \textit{Invoice}.

\lstset{caption={An example of a unit test},
label={unit-test}}
\begin{lstlisting}
@Test
public void shouldCalculateTaxes() {
	Invoice inv = new Invoice("John", 50.0);

	double tax = inv.calcTaxes(false);
	double biggerTax = inv.calcTaxes(true);
		
	assertEquals(50 * 0.06, tax);
	assertEquals(50 * 0.07, biggerTax);
}
\end{lstlisting}

A few particular cases are worth mentioning. We are interested in the number 
of distinct unit tests per production method. Hence, calling the same production method
more than once inside the same unit test does not cause any difference in our metric 
(even if input parameters differ). For instance, in our last example, the 
method \textit{calculateTaxes()} is invoked twice, but we mark it 
as being tested by \textit{shouldCalculateTaxes()}.

However, if a production method makes use of many other methods in its implementation,
we mark all of these methods as tested by the current unit test. In Listing \ref{calculate-taxes-impl},
we show an example of the implementation of \textit{calculateTaxes()}. In this case, due to the 
invocation \textit{inc.calculateTaxes(false)} inside the test \textit{shouldCalculateTaxes()}, 
we mark \textit{calculateTaxes()}, \textit{taxA()}, and \textit{taxB()} as tested 
by such unit test. Furthermore, due to the invocation \textit{inv.calculateTaxes(true)} inside
the same unit test, we additionally link the production method \textit{taxC()} to such test.

In addition, suppose that a class C has a method M1 and this method calls method M2. But, it only calls M2 if a certain condition N is true. 
Since we are using static analysis, our approach will count M2 as a covered method, regardless the value of N. 
That can be seen in Listing \ref{calculate-taxes-impl}: if no test passes \textit{increment} as true,
\textit{taxC();} will never be invoked. Our approach does not notice that, 
assuming that \textit{taxC()} was tested.

\lstset{caption={Internal implementation of calculateTaxes()}, label={calculate-taxes-impl}}
\begin{lstlisting}
public double calcTaxes(double increment) {
	double taxA = taxA();
	double taxB = taxB();
	double sum = taxA + taxB;
	if (increment) sum += taxC();
	return sum;
}
\end{lstlisting}

Code coverage can be calculated on different levels, such as method level, class level, or even
system level. Due to space constraints, in the paper we focus on the code coverage at the class level as a mean of our proof of concept.
We conceived our heuristic as follows (Equation \ref{formula-doida}): where for a certain class $C$ with methods $m_1, ..., m_n$, the code coverage of C is the ratio of the number of unit tests in the suite that test a certain method from C and the sum of McCabe's number from each method inside C. In the formula, CC stands for Cyclomatic Complexity.

\begin{equation}
\label{formula-doida}
Cov(C) = min(1,\frac{\sum_{i=1}^{n}min(UnitTests(m_i), CC(m_i))}{\sum_{i=1}^{n}{CC(m_i)}})
\end{equation}

Suppose that the class \textit{Invoice} has McCabe's number equals to 10, 
and there are 6 unit tests that invoke methods that belong to it. 
In this case, 6 / 10 = 0.6, meaning  60\% of code coverage.
All data collected in this study is freely available\footnote{https://github.com/mauricioaniche/msr2014}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment Design}
\label{sec-experiment}

To validate the heuristic, we compare the code coverage calculated to the results of Emma - which is one of the most used code coverage tool.
We analyze 3 Java projects developed by a Brazilian software company. Two of them are
web applications and one of them is a console application. In Table \ref{tab:projects}, we describe
some characteristics of the projects.

\begin{table}[h!]
\centering
\caption{Number of classes and methods per project}
\begin{tabular}{ | l | r | r | r | }
\hline
& Classes & Methods & Unit Tests\\ 
\hline
Gnarus & 769 & 1823 & 826\\ 
MetricMiner & 225 & 1009 & 341\\ 
Tubaina & 261 & 372 & 298\\ 

\hline
\end{tabular}
\label{tab:projects}
\end{table}

We decided to use the following formula (\ref{formula-doida-diff}) to proceed with the analysis:

\begin{equation}
\label{formula-doida-diff}
diff = ourCoverage - emmaCoverage
\end{equation}

Where every time that diff variable reaches zero, it means that both numbers
were equal, and the heuristic worked as expected. A negative number indicates that the heuristic calculated a smaller number than the real; a positive number indicates that the heuristic calculated a higher
number.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of concept}
\label{poc}

We created an aspect to the codebase that prints all methods that were invoked by a unit test. 
With this information, we were able to calculate the number of unit tests per production method. 
We then applied the formula. 

In Figures \ref{fig:metricminer-aj}, \ref{fig:tubaina-aj}, and
\ref{fig:gnarus-aj}, we show the results we obtained for MetricMiner, Tubaina, and Gnarus, respectively. 

In MetricMiner, we can notice that 47\% of the data is zero. It means that the heuristic
presents a close approximation of the real coverage. Descriptive statistics also show the same: 
the median of the distribution is 0 and the mean is 0.1149. The first quartile is 0 and the
third quartile is 0.2414.

In Tubaina project, 62\% of the data is zero. The median is 0 and the mean is 0.0790. The first quartile is
0 and the third quartile is 0.1667. 
In Gnarus project, we can see that 61\% of the data is zero. The median is 0, the mean is -0.0152. The first quartile is 0 and the third quartile is 0.

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.35]{../stats/metricminer-histograma-aspectj.png}
  \caption{The dynamic implementation of the heuristic compared to Emma's in the MetricMiner}
  \label{fig:metricminer-aj}
\end{figure}

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.35]{../stats/tubaina-histograma-aspectj.png}
  \caption{The dynamic implementation of the heuristic compared to Emma's in the Tubaina}
  \label{fig:tubaina-aj}
\end{figure}

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.35]{../stats/gnarus-histograma-aspectj.png}
  \caption{The dynamic implementation of the heuristic compared to Emma's in the Gnarus}
  \label{fig:gnarus-aj}
\end{figure}

Based on the numbers we obtained, we believe that the heuristic has an acceptable performance, depending on the study being conducted. 
In particular, if we look at the third quartile for all projects, we can notice that the number is
still small, indicating that the heuristic looks acceptable. Because of that,
we decided to implement the metric using static analysis. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Static Analysis}
\label{sec-results}

We implemented\footnote{https://github.com/mauricioaniche/gelato2} a static tool
to calculate code coverage. Static analysis misses some relations found in dynamic analysis, like those related to polymorphism.

% Gustavo: consegue ver outra forma de descrever isso?
In Figure \ref{fig:metricminer}, we show the histogram of the difference between Emma and our static approach to
the MetricMiner project. By looking to this figure, one can notice that 63\% of the data is zero. 
The median of the distribution is 0 and the mean is -0.1226. The first quartile is 0 and the
third quartile is also 0.

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.3]{../stats/metricminer-histograma-gelato.png}
  \caption{Our static implementation compared to Emma's in the MetricMiner}
  \label{fig:metricminer}
\end{figure}

In Figure \ref{fig:tubaina}, from Tubaina project, 45\% of the data is zero. 
However, 48\% of the classes are between -0.2 and -1. Around 10\% are between -0.2 and -0.5.
The median is -0.1603 and the mean is -0.3074. The first quartile is
-0.6 and the third quartile is 0.

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.3]{../stats/tubaina-histograma-gelato.png}
  \caption{Our static implementation compared to Emma's in the Tubaina}
  \label{fig:tubaina}
\end{figure}


In Figure \ref{fig:gnarus}, from Gnarus project, we can see that the data are more varied as 
compared to the previous cases. Only 27\% is zero. Around 50\% are between -0.5 and -1. 
The median is also higher than the others: -0.3852. The mean is -0.3272. The first quartile
is -0.5 and the third quartile is 0.

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.3]{../stats/gnarus-histograma-gelato.png}
  \caption{Our static implementation compared to Emma's in the Gnarus}
  \label{fig:gnarus}
\end{figure}

By looking at the numbers above, we notice that the implementation result was fairly good in the first project, in which the median was 0, regular in the second one, in which the median was -0.16, and worse in the last, as the median was -0.38.

We decided to investigate why some classes had such a bad performance. We found out
that our tool was not correctly interpreting some cases. As an example, 
the current implementation does not identify inline invocations with lists, such
as \textit{list.get(0).doX()}. In that case, the invocation of \textit{doX()} is not identified.

% GUSTAVO: revisa e completa se achar necessario?
Although there were a few implementation problems, we believe that this is a valid approach
when mining repositories. We believe that, with some effort on the tool,
we will be able to use it to analyze large sets of projects.


\section{Threats to Validity}
\label{sec-threats}

There are a few threats to the validity of this research. Our implementation still does not get all cases: implementing a parser is not an easy task. However, as we showed, the 
heuristic itself seems valid.

We only evaluated three projects that belong to the same company. It means that these projects
follow basically the same structure, development, and testing rules. In a future work, we will run the experiment in many different projects.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec-related-work}

Many studies discuss the importance of code coverage when analyzing code quality. 
Sebastian \textit{et al.} \cite{sebastian} say that many software development practices and tools
are based on this number. However, they argue that developers usually calculate it for a single
version of the system, and perform analysis on future versions without recalculating the numbers.
He shows that even relatively small modifications on the source code affect the code coverage, and the impact of the change on the metric is hard to predict.

To the best of our knowledge, there is only one study that discusses the calculation of
code coverage by means of static analysis. Tiago and Visser \cite{tiago} propose a
technique that uses slicing of static call graphs to estimate the
dynamic test coverage. 
In their approach, they define method coverage as the ratio between covered 
and defined methods per class. They showed that, at system level, the approach
proved to be satisfactory. In package and class level, the difference between
the real and the calculated coverage was small in most cases.

This approach is different from ours. We use McCabe's number to estimate the
minimum number of tests that a class should have. We are also more interested on calculating
code coverage at the class level (from which is possible to derive the system-level number). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
\label{sec-conclusion}

Code coverage is an important metric to analyze software maintenance and evolution. However, it is not easy to be calculated in large scale, as most tools require compiled code. In this study, we conceived and implemented a heuristic to calculate code coverage using static analysis. In the preliminary evaluation, it produced results similar to those produced by the popular code coverage tool Emma for two of the three projects analyzed. 

As future work, we intend to improve our implementation in order to
achieve better results. Once the desired level of performance is achieved, we intend to implement code coverage calculation at other levels of abstraction: method, package, and system. Finally, we also envision running this same study on more projects from different companies and domains.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgments}

We thank \textit{Caelum Ensino e Inovação} for allowing us to run the study in its environment,
as well as supporting the development of the tool. We also thank Gustavo Pinto, from UFPE, who
gave us a valuable feedback on this text. We also thank NAWEB, CNPq, and FAPESP for their support. 

\bibliographystyle{abbrv}
\bibliography{sigproc}

\end{document}
