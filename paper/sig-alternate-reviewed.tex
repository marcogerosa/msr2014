%Aqui voce mostra que a heurística em si funciona (isto é, tem resultados parecidos com o do Emma). Eu acho que
%primeiro eu mostraria a heurística em si pra mostrar que o resultado perfeito (e inalcançável) é satisfatório quando
%comparado ao Emma (porque se não fosse, nem a melhor implementação teria resultado satisfatório). Em seguida, eu mostraria 
%a tua implementação. Depois, eu tentaria mostrar o quão 
%perto ou longe está o cálculo do numerador da heurística (link unit test - production method) na tua implementação vs o aspecto (Listing 5). 
%Bastaria mostrar precision e recall pra cada projeto. Lógico que, por conta de polimorfismo, etc, a implementação nunca será perfeita. 
%Mas saber o quão perto ou o quão longe está, seria interessante. Esse no fundo é o desafio principal por detrás do calculo da cobertura.
%Feito isso, aí sim eu rodaria nos 3 projetos e mostraria os resultados. Em resumo você começa num nível mais abstrato e termina na tua implementação
%(que revela o melhor que a gente consegue fazer na prática por enquanto)

\documentclass{sig-alternate}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{flushend}

\usepackage{xcolor}
\usepackage[normalem]{ulem}

\lstset{
language=Java,
basicstyle=\footnotesize,
showtabs=false, 
showspaces=false,
showstringspaces=false,
frame=single,
breaklines=true,
tabsize=2,
captionpos=b
}

\begin{document}

\conferenceinfo{MSR}{Mining Software Repositories '14}

\title{Calculating Code Coverage Without Running the Tests:\\
A Heuristic Based on Static Code Analysis}

\numberofauthors{1}
\author{
\alignauthor
Mauricio F. Aniche, Gustavo A. Oliva, Marco A. Gerosa\\
\affaddr{Institute of Mathematics and Statistics}\\
\affaddr{University of São Paulo}\\
\email{\{aniche, goliva, gerosa\}@ime.usp.br}
}

\maketitle
\begin{abstract}

Code coverage is an important metric for software evolution and maintenance.
However, computing it requires building the project and executing its test suite.
This \textcolor{red}{approach is very time consuming and then,} unfeasible for large-scale mining software repositories studies. To this end,
we conceived and implemented a heuristic to calculate code coverage based on 
code static analysis. In this paper, we describe the heuristic, its implementation, 
and a preliminary evaluation on three industry projects. We evaluated our implementation by 
comparing its results to those produced by the popular code coverage tool \textcolor{red}{called} Emma. \xout{Results showed that, 
although our implementation is not complete yet, its results were at least regular for two of the projects.}

\textcolor{red}{Preliminary results showed that our novel approach has similar results when compared to the state-of-the-use code coverage software: 2 out of 3 of the analyzed projects were successfully applied. We also observe X, Y and Z. \textbf{Here I'd like to see more concrete results. For instance, the overall time spent is less than X..}}

These promising results showed \xout{that it might be suitable to use} \textcolor{red}{the suitability of our} approach for mining \textcolor{red}{software repository} studies.


\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

\xout{One advantage of studies that make use of mining software repository is to techniques
is that they can leverage a large quantity of projects and data. This huge
amount of information allows researchers to validate hypotheses with more property.}


\textcolor{red}{One advantage of mining software repository studies is to help } researchers \textcolor{red}{in} extract \textcolor{red}{and understand} data from \xout{many} different sources, 
such as source code repositories (a.k.a. version control systems), 
issue trackers, mailing lists, and so on. Most of these repositories 
store textual information, which is often easy to extract but hard to process. 
In the case of source code mining, a particular problem arises \textcolor{red}{when} \xout{some} metrics 
require the code to be compiled first. \xout{And} Compiling code is \textcolor{red}{not a straightforward} \xout{a complicated} task. \textcolor{red}{For instance,} 
each project has its own build process and \textcolor{red}{might} required \textcolor{red}{external} libraries. Furthermore, 
it is common that many of these libraries are not available by the time \textcolor{red}{that} 
researchers obtain the source code \textcolor{red}{[ref]}. Therefore, compiling and executing 
unit tests may not be feasible when conducting \textcolor{red}{a large scale} mining \textcolor{red}{software repository} stud\textcolor{red}{y}. \xout{involving a large quantity of projects.} \textcolor{red}{This is a well-known problem in the software engineering research. Some authors have spent a significant effort on it[REF1, REF2, REF3], but no consensus has emerged from these discussions.}

An example of \textcolor{red}{such metric is} \xout{code metric that usually requires compiled code is} \textbf{code coverage}. 
This metric defines the percentage of production code that is covered 
by the test suite. To enable the calculation of code coverage for mining software
repository studies, we conceived a heuristic that does not require compiled code
and neither requires the test suite to be run. In this paper, we describe this heuristic,
its implementation, and its evaluation on three industry projects. 
We found out that, although our tool implementation is not fully completed yet, the 
proposed heuristic on how to calculate code coverage using static analysis seems valid. \textcolor{red}{I would be nice if you can use more details here. What are your main contributions? State they here!}


\textcolor{red}{\textbf{In my opinion, something is missing in this introduction. For instance, which are the state-of-the-art studies on software metrics? Does anyone of them suggest a similar approach? If so, what is wrong with those existing solutions? Then, why and how is your approach better then those ones? The way that I work in introductions is: if you have something important to say, say it as upfront as possible.}}


This paper is structured as follows. In Section \ref{sec-code-coverage}, 
we introduce the concept of code coverage. In Section \ref{sec-heuristic}, 
we present our heuristic. In Section \ref{sec-experiment}, we describe the 
experiment design. In Section \ref{poc}, we describe a proof of concept that
we have done prior to implement the static tool. 
In Section \ref{sec-results}, we present the results we 
obtained and discuss them. In Section \ref{sec-threats}, 
we present the threats that may have influenced the validity of our study. 
In Section \ref{sec-related-work}, we show related studies and compare them 
to our approach. Finally, in Section \ref{sec-conclusion}, we state our 
conclusions and plans for future work. \textcolor{red}{This paragraph could be easily cuttable if you are running out of space}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\xout{Code Coverage} Background}
\label{sec-code-coverage}

% Gustavo: tentei mostrar estudos relacionados aqui, mas acho que poderia ter vendido melhor
Code coverage measures the degree to which production code is tested by a test suite\textcolor{red}{[REF]}. 
When a unit test exercises a piece of code, developers say that such piece
of code is covered. Many studies argue that code coverage is an important
metric when dealing with software maintenance and evolution \cite{sebastian, del-frate, mei-hwa}.

The common output of the tools \textcolor{red}{that implement such metric} is a percentage \textcolor{red}{value}, which indicates the fraction of production code \textcolor{red}{of} lines, instructions, 
or even execution branches that are exercised by the test suite. Such information can be shown according to different
degrees of abstraction, such as per package, \xout{per} compilation unit, \xout{per} class, or \xout{per} method. 

Emma \footnote{http://emma.sourceforge.net} and Cobertura\footnote{http://cobertura.github.io/cobertura} are popular 
code code coverage tools. \xout{They both} \textcolor{red}{Both of them} use dynamic analysis to calculate the metric. They instrument the code, execute 
the unit test suite, and then calculate the degree to which the production code was exercised by the test suite. \textcolor{red}{These tools have to main problems: (i) depending on the size of the test suite, it can take a long time to compute the metric, then they are unfeasible for large scale studies, and (ii) they require compiled code, and as we said before, compiling large amount of projects is still an open research topic}. \xout{Besides 
taking a long time to calculate (as they need to execute the entire test suite),
they require compiled code. As said before, when analyzing a large quantity of repositories,
compiling the code may not be an option.} Researchers \textcolor{red}{then} need an alternative way to calculate code coverage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\xout{The Proposed Heuristic} Our Approach}
\label{sec-heuristic}

Our heuristic is based on the idea that if a production method has $N$ different branches, then it should
have at least $N$ associated unit tests. \xout{As we wanted to avoid dynamic analysis, the only option was 
to apply static analysis to the source code.} The first step \xout{was} \textcolor{red}{is} to estimate the number of branches 
a production class has. To do that, we decided to use McCabe's cyclomatic complexity \cite{mccabe}, a popular
metric on academia and industry.
McCabe's number \xout{basically} shows the different number of execution paths per method. 
As we are calculating code coverage in class method, we calculate the class' cyclomatic complexity
by summing up McCabe's number of all methods.

The next step was to count the number of unit tests that invoke each production method. To accomplish that
in a static way, we search for production method invocations that happen inside each unit test. As an example, in Listing
\ref{unit-test}, the invocation \textit{inv.calcTaxes()} links the surrounding unit test (\textit{shouldCalculateTaxes()}) to 
the production method \textit{calcTaxes(double increment)} from the class \textit{Invoice}.

\lstset{caption={An example of a unit test},
label={unit-test}}
\begin{lstlisting}
@Test
public void shouldCalculateTaxes() {
	Invoice inv = new Invoice("John", 50.0);

	double tax = inv.calcTaxes(false);
	double biggerTax = inv.calcTaxes(true);
		
	assertEquals(50 * 0.06, tax);
	assertEquals(50 * 0.07, biggerTax);
}
\end{lstlisting}

There are a few particular cases that are worth mentioning. We are interested in the number 
of distinct unit tests per production method. Hence, calling the same production method
more than once inside \textcolor{red}{the same} unit test does not \textcolor{red}{cause any difference in our metric}
\xout{make the relationship between the method 
and the test any different} (even if input parameters differ). \textcolor{red}{For instance,} in \textcolor{red}{our last example} \xout{the example above}, the 
method \textit{calculateTaxes()} is invoked twice, but the only thing we do is mark it 
as being tested by \textit{shouldCalculateTaxes()}.

However, if a production method makes use of many other methods in its implementation,
we mark all of these methods as tested by the current unit test. In Listing \ref{calculate-taxes-impl},
we show an example of the implementation of \textit{calculateTaxes()}. In this case, due to the 
invocation \textit{inc.calculateTaxes(false)} inside the test \textit{shouldCalculateTaxes()}, 
we mark \textit{calculateTaxes()}, \textit{taxA()}, and \textit{taxB()} as tested 
by such unit test. Furthermore, due to the invocation \textit{inv.calculateTaxes(true)} inside
the same unit test, we additionally link the production method \textit{taxC()} to such test.

\textcolor{red}{Maybe I missed something, but what if a class C has a method M1. And, method M1 calls method M2. But, it only calls M2 if a certain condition N is true. Since you are using static analysis, I believe that your approach counts M2 as a covered method, regardless the value of N. If so, it could be interesting to describe some false-positives of your a`pproach.}

\lstset{caption={Internal implementation of calculateTaxes()}, label={calculate-taxes-impl}}
\begin{lstlisting}
public double calcTaxes(double increment) {
	double taxA = taxA();
	double taxB = taxB();
	double sum = taxA + taxB;
	if (increment) sum += taxC();
	return sum;
}
\end{lstlisting}

% Gustavo: gostou da explicacao?
Code coverage can be calculated on different levels, such as method level, class level, or even
system level. \xout{As this work is part of a full research, in there, we are interested in} \textcolor{red}{Due space constrains, in this paper we focus on the} code coverage at the class level \textcolor{red}{as a mean of our proof of concept}.
\xout{Because of that,} We conceived our heuristic as follows (Equation \ref{formula-doida}):

\begin{equation}
\label{formula-doida}
Cov(C) = min(1,\frac{\sum_{i=1}^{n}min(UnitTests(m_i), CC(m_i))}{\sum_{i=1}^{n}{CC(m_i)}})
\end{equation}

\textcolor{red}{where} for a certain class $C$ with methods $m_1, ..., m_n$, 
the code coverage of C is the ratio of the number of unit tests in the suite that test a certain method from C and 
the sum of McCabe's number from each method inside C. \textcolor{red}{What is CC?}

Suppose that the class \textit{Invoice} has McCabe's number equals to 10, 
and there are 6 unit tests that invoke methods that belong to it. 
In this case, 6 / 10 = 0.6, which means 60\% of code coverage.
All data collected in this study is freely available\footnote{https://github.com/mauricioaniche/msr2014}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiment Design}
\label{sec-experiment}

To validate the heuristic, we \xout{decided to} compare the code coverage calculated by \xout{it} our approach with the ones 
calculated by a Emma \textcolor{red}{-- which is one of the most used code coverage tool}. \xout{We chose Emma as we had a previous experience with it.} We
\xout{had} \textcolor{red}{analyze} 3 \textcolor{red}{Java} projects \xout{as subjects, all of them} developed by \textcolor{red}{a Brazilian software} \xout{same} company\xout{, in Java}. Two of them are
web applications and one of them is a console application. \textcolor{red}{The source code of these projects are not available due XXXX restrictions, but,} in Table \ref{tab:projects}, we describe \textcolor{red}{some characteristics of them}
\xout{the size of each one.} \textcolor{red}{\textbf{If possible, I would recommend you guys to analyze more projects (OSS works good here). I mean, the more, the better.}}


\begin{table}[h!]
\centering
\caption{Quantity of classes and methods per project}
\begin{tabular}{ | l | r | r | r | }
\hline
& Classes & Methods & Unit Tests\\ 
\hline
Gnarus & 769 & 1823 & 826\\ 
MetricMiner & 225 & 1009 & 341\\ 
Tubaina & 261 & 372 & 298\\ 

\hline
\end{tabular}
\label{tab:projects}
\end{table}

\xout{Once we obtained the code coverage numbers (at the class level) reported by Emma and our tool, 
we compared them to see how different they were. We used histograms and descriptive statistics. }

\xout{To do the analysis,} We decided to use the \textcolor{red}{following formula to proceed with the analysis:}

$ diff = ourCoverage - emmaCoverage$

\xout{number calculated by our heuristic minus the
number calculated by Emma. It means}

\textcolor{red}{Where every} \xout{that every} time \xout{we find a} \textcolor{red}{that diff variable reaches} zero, it means that both numbers
were equal, and the heuristic \textcolor{red}{works as expected} \xout{was perfect}. A negative number indicates that the heuristic calculated
a smaller number than the real; a positive number indicates that the heuristic calculated a higher
number.

\xout{In the following sections, we first describe a proof of concept, and then present the results found
with the implemented static analysis tool.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Concept}
\label{poc}

First of all, we decided to check whether the heuristic made sense. To do that, we 
created an aspect to the codebase that prints all methods that were invoked by a unit test. 
With this information, we were able to calculate the number of unit tests per production method. 
We then applied the formula. \textcolor{red}{I guess this paragraph may fits better in the threat to validity section}

In Figures \ref{fig:metricminer-aj}, \ref{fig:tubaina-aj}, and
\ref{fig:gnarus-aj}, we show the results we obtained for MetricMiner, Tubaina, and Gnarus, respectively. \textcolor{red}{Why you don't put all the three figures in one? They are in the same scale, and it will save you some space}

% TODO: discutir melhor as porcentagens aqui tb, ver no R os valores
In MetricMiner, we can notice that 47\% of the data is zero. It means that the heuristic
presents a \textcolor{red}{close} \xout{was a perfect} approximation of the real \textcolor{red}{coverage}\xout{value}. Descriptive statistics also show the same: 
the median of the distribution is 0, and the mean is 0.1149. The first quartile is 0, and the
third quartile is 0.2414. \textcolor{red}{Here you can be more deeper. I mean, if you say that values between -.2 to .2 are ``good enough" approximation, you can say that your approach is up to 70\% close to the state-of-the-practice. You can also derive more metrics, such as: the better one: 0, good enough: -.2 to .2, and workable: -.4 to .4. If so, you can also say that your approach is flexible, or configurable.}

In Tubaina project, 62\% of the data is zero. The median is 0, and the mean is 0.0790. The first quartile is
0, and the third quartile is 0.1667. 
In Gnarus project, we can see that 61\% of the data is zero. The median is 0, the mean is -0.0152. The first quartile
is 0, and the third quartile is 0.

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.35]{../stats/metricminer-histograma-aspectj.png}
  \caption{The dynamic implementation of the heuristic compared to Emma's in the MetricMiner}
  \label{fig:metricminer-aj}
\end{figure}

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.35]{../stats/tubaina-histograma-aspectj.png}
  \caption{The dynamic implementation of the heuristic compared to Emma's in the Tubaina}
  \label{fig:tubaina-aj}
\end{figure}

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.35]{../stats/gnarus-histograma-aspectj.png}
  \caption{The dynamic implementation of the heuristic compared to Emma's in the Gnarus}
  \label{fig:gnarus-aj}
\end{figure}

Based on the numbers we obtained, we believe that the heuristic has an acceptable performance. 
In particular, if we look at the third quartile from all of them, we can notice that the number is
still small, indicating that the heuristic looks acceptable. Because of that,
we decided to implement the metric using static analysis. 

\textcolor{red}{What about a performance overview of your approach? How long it take to calculate those projects? And, can you compare the time took by your approach an emma?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Static Analysis}
\label{sec-results}

\textcolor{red}{The next step of our work is to provide a more reliable (ou algum outro adjetivo) view using static analyzis.} 
\xout{As we were satisfied with the results on the proof of concept,} We \textcolor{red}{then} implemented\footnote{Freely available at https://github.com/mauricioaniche/gelato2} a static tool
to calculate code coverage. It is known that static analysis will never achieve the same performance
found in dynamic analysis: static analysis cannot deal with polymorphism and other concepts from object-orientation. \textcolor{red}{Be careful with never, always, perfect and similar words.}
\xout{However, as said before, static analysis is important when dealing with a large set of projects.
The tool we implemented is freely available on Github}

% Gustavo: consegue ver outra forma de descrever isso?
In Figure \ref{fig:metricminer}, we show the histogram of the difference between Emma and our static approach to
the MetricMiner project. By looking to this figure, one can notice that 63\% of the data is zero. 
The median of the distribution is 0, and the mean is -0.1226. The first quartile is 0, and the
third quartile is also 0.

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.3]{../stats/metricminer-histograma-gelato.png}
  \caption{Our static implementation compared to Emma's in the MetricMiner}
  \label{fig:metricminer}
\end{figure}

In Figure \ref{fig:tubaina}, from Tubaina project, 45\% of the data is zero. 
However, 48\% of the classes are between -0.2 and -1. Around 10\% are between -0.2 and -0.5.
The median is -0.1603, and the mean is -0.3074. The first quartile is
-0.6, and the third quartile is 0.

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.3]{../stats/tubaina-histograma-gelato.png}
  \caption{Our static implementation compared to Emma's in the Tubaina}
  \label{fig:tubaina}
\end{figure}


In Figure \ref{fig:gnarus}, from Gnarus project, we can see that the data are more varied as 
compared to the previous cases. Only 27\% is zero. Around 50\% are between -0.5 and -1. 
The median is also higher than the others: -0.3852. The mean is -0.3272. The first quartile
is -0.5, and the third quartile is 0.

\begin{figure}[h!H]
  \centering
  \includegraphics[scale=0.3]{../stats/gnarus-histograma-gelato.png}
  \caption{Our static implementation compared to Emma's in the Gnarus}
  \label{fig:gnarus}
\end{figure}

By looking to the numbers above, we notice that the implementation was good in the first project,
in which the median was 0, regular in the second one, in which the median was -0.16. However,
in the last project, the heuristic did not go well, as the median was -0.38.

We decided to closely investigate why some classes had such a bad performance. We found out
that our tool was not correctly interpreting a few expressions. As an example, 
the current implementation does not identify inline invocations with lists, such
as \textit{list.get(0).doX()}. In that case, the invocation of \textit{doX()} is not identified. \textcolor{red}{Is it possible to fix due deadline time?}

% GUSTAVO: revisa e completa se achar necessario?
Although there were a few implementation problems, we believe that this is a valid approach
when mining repositories. The numbers showed, both from the proof of concept and from the
static tool, that the heuristic is acceptable. We believe that, with some effort on the tool,
we will be able to use it to analyse large sets of projects. \textcolor{red}{I would strongly recommend a performance comparison between your approach and emma. How much time a researcher will save when using your approach? Another questions: How easy to use is your approach? Your heuristic seems to be more flexible. I mean, you can use it in projects written in ruby, scala, etc. You can sell it better.}


\section{Threats to Validity}
\label{sec-threats}

There are a few threats to the validity of this research. The first one is that our implementation
still does not fully work: implementing a parser is not an easy task. However, as we showed, the 
heuristic itself seems valid. 

We only evaluated three projects that belong to the same company. It means that these projects
follow basically the same structure, development, and testing rules. That may bias the results.
In a future work, we should run the experiment in many different projects.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec-related-work}

Many studies discuss the importance of code coverage when analyzing code quality. 
Sebastian \textit{et al.} \cite{sebastian} say that many software development practices and tools
are based on this number. However, they argue that developers usually calculate it for a single
version of the system, and perform analysis on future versions without recalculating the numbers.
He shows that even relatively small modifications on the source code can affect the code coverage, 
and the impact of the change on the metric is hard to predict.

To the best of our knowledge, there is only one study that discusses the calculation of
code coverage through static analysis. Tiago and Visser \cite{tiago} propose a
technique that uses slicing of static call graphs to estimate the
dynamic test coverage. \textcolor{red}{Cite this guy in the introduction}
In their approach, they define method coverage as the ratio between covered 
and defined methods per class. They showed that, at system level, the approach
proved to be satisfactory. In package and class level, the difference between
the real and the calculated coverage was small in most cases.

This approach is slightly different from ours. We use McCabe's number to estimate the
minimum number of tests that a class should have. We are also more interested on calculating
code coverage at the class level (from which is possible to derive the system-level number). 
\xout{Also, creating a call graph is not simple and takes more effort than just counting
the invoked methods.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
\label{sec-conclusion}

Code coverage is an important metric to analyze software maintenance and evolution. However, it is 
not easy to be calculated, as most tools require compiled code. In this study, we conceived, implemented, 
and evaluated a heuristic to calculate code coverage using static analysis. 
\textbf{Apparently, our heuristic can be used in mining software repositories studies}, 
as its results were similar to those produced by the popular code coverage tool Emma. 

As future work, we intend to fix existing bugs in our implementation in order to
achieve better results. Once the desired level of performance is achieved, we intend to implement code coverage
calculation at other levels of abstraction: method, package, and system. Finally, we also 
envision running this same study on more projects from different companies and domains.

% Gustavo: acha que expliquei bem?
\xout{In addition, in a previous study \cite{aniche-csmr}, 
we found out that a few developers prefer to write a many different checks in a single
unit test. In our heuristic, even though a method is invoked more than once in a unit test, we count it 
as only one test. A future evaluation would be to consider that the same method can be tested more than once
in a single unit test. That may affect the results, and it needs to be validated.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgments}

We thank \textit{Caelum Ensino e Inovação} for allowing us to run the study in its environment,
as well as supporting the development of the tool. \textcolor{red}{You don't need to acknowledge no one in the submitted version. It may save you some important space.}

\bibliographystyle{abbrv}
\bibliography{sigproc}

\end{document}
